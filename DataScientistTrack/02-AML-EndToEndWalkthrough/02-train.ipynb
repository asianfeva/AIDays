{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on GPU cluster\n",
    "\n",
    "![AML Arch](images/amlarch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure remote training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AML workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get_workspace error using subscription_id=952a710c-8d9c-40c1-9fec-f752138cc0b3, resource_group_name=jkaml, workspace_name=jkaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace not found. Creating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The resource group doesn't exist or was not provided. AzureML SDK is creating a resource group=jkaml in location=southcentralus using subscription=952a710c-8d9c-40c1-9fec-f752138cc0b3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote the config file config.json to: /data/home/demouser/notebooks/AIDays/DataScientistTrack/02-AML-EndToEndWalkthrough/aml_config/config.json\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "subscription_id ='952a710c-8d9c-40c1-9fec-f752138cc0b3'\n",
    "resource_group ='jkaml'\n",
    "workspace_name = 'jkaml'\n",
    "\n",
    "try:\n",
    "   ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "\n",
    "   print('Workspace configuration succeeded. You are all set!')\n",
    "except:\n",
    "   print('Workspace not found. Creating...')\n",
    "   workspace_region = 'southcentralus'\n",
    "   ws = Workspace.create(name = workspace_name,\n",
    "                subscription_id = subscription_id,\n",
    "                resource_group = resource_group, \n",
    "                location = workspace_region,\n",
    "                create_resource_group = True,\n",
    "                exist_ok = True)\n",
    "\n",
    "ws.get_details()\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script\n",
    "\n",
    "To submit the job to the cluster, first create a training script. Run the following code to create the training script called `train.py` in the directory you just created. The scripts encapsulates the code we prepared in the first part of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import resnet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "    \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Input\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "\n",
    "\n",
    "# This is a generator that yields batches of preprocessed images\n",
    "class ImageGenerator(tf.keras.utils.Sequence):    \n",
    "    \n",
    "    def __init__(self, img_dir, preprocess_fn=None, batch_size=64):\n",
    "        \n",
    "        # Create the dictionary that maps class names into numeric labels \n",
    "        folders = os.listdir(img_dir)\n",
    "        folders.sort()\n",
    "        indexes = range(len(folders))\n",
    "        label_map = {key: value for (key, value) in zip(folders, indexes)}\n",
    "        self.num_classes = len(label_map)\n",
    "        \n",
    "        # Create a list of all images in a root folder with associated numeric labels\n",
    "        labeled_image_list = [(os.path.join(img_dir, folder, image), label_map[folder]) \n",
    "                              for folder in folders \n",
    "                              for image in os.listdir(os.path.join(img_dir, folder))\n",
    "                              ]\n",
    "        # Shuffle the list\n",
    "        random.shuffle(labeled_image_list)\n",
    "        # Set image list and associated label list\n",
    "        self.image_list, self.label_list = zip(*labeled_image_list) \n",
    "        # Set batch size\n",
    "        self.batch_size = batch_size\n",
    "       \n",
    "        # Set the pre-processing function passed as a parameter\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        \n",
    "        # Set number of batches\n",
    "        self.n_batches = len(self.image_list) // self.batch_size\n",
    "        if len(self.image_list) % self.batch_size > 0:\n",
    "            self.n_batches += 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.n_batches\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        pathnames = self.image_list[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        images = self.__load_images(pathnames)\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    # Load a set of images passed as a parameter into a NumPy array\n",
    "    def __load_images(self, pathnames):\n",
    "        images = []\n",
    "        for pathname in pathnames:\n",
    "            img = image.load_img(pathname, target_size=(224,224,3))\n",
    "            img = image.img_to_array(img)\n",
    "            images.append(img)\n",
    "        images = np.asarray(images)\n",
    "        if self.preprocess_fn != None:\n",
    "            images = self.preprocess_fn(images)   \n",
    "        \n",
    "        return images\n",
    "    \n",
    "    # Return labels in one-hot encoding\n",
    "    def get_labels(self):\n",
    "        \n",
    "        return to_categorical(np.asarray(self.label_list), self.num_classes)\n",
    "    \n",
    "\n",
    "def fcn_classifier(input_shape=(2048,), units=512, classes=6,  l1=0.01, l2=0.01):\n",
    "    features = Input(shape=input_shape)\n",
    "    x = Dense(units, activation='relu')(features)\n",
    "    x = Dropout(0.5)(x)\n",
    "    y = Dense(classes, activation='softmax', kernel_regularizer=l1_l2(l1=l1, l2=l2))(x)\n",
    "    model = Model(inputs=features, outputs=y)\n",
    "    model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_evaluate(run):\n",
    "   \n",
    "    # Create bottleneck featurs\n",
    "    train_images_dir = os.path.join(FLAGS.data_folder, 'train')\n",
    "    valid_images_dir = os.path.join(FLAGS.data_folder, 'valid')\n",
    "\n",
    "    train_generator = ImageGenerator(train_images_dir, resnet50.preprocess_input)\n",
    "    valid_generator = ImageGenerator(valid_images_dir, resnet50.preprocess_input)\n",
    "\n",
    "    featurizer = resnet50.ResNet50(\n",
    "                weights = 'imagenet', \n",
    "                input_shape=(224,224,3), \n",
    "                include_top = False,\n",
    "                pooling = 'avg')\n",
    "\n",
    "    print(\"Generating bottleneck features\")\n",
    "    train_features = featurizer.predict_generator(train_generator, verbose=1)\n",
    "    train_labels = train_generator.get_labels()\n",
    "\n",
    "    valid_features = featurizer.predict_generator(valid_generator, verbose=1)\n",
    "    valid_labels = valid_generator.get_labels()\n",
    "    \n",
    "    # Create a classifier\n",
    "    model = fcn_classifier(input_shape=(2048,), units=FLAGS.units, l1=FLAGS.l1, l2=FLAGS.l2)\n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting training\")\n",
    "    model.fit(train_features, train_labels,\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          shuffle=True,\n",
    "          validation_data=(valid_features, valid_labels))\n",
    "          \n",
    "    # Save the trained model to outp'uts which is a standard folder expected by AML\n",
    "    print(\"Training completed.\")\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    model_file = os.path.join('outputs', run.run_id + '.hd5')\n",
    "    #model_file = os.path.join('outputs', FLAGS.run_id + '.hd5')\n",
    "    print(\"Saving model to: {0}\".format(model_file))\n",
    "    model.save(model_file)\n",
    "    \n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# Default global parameters\n",
    "tf.app.flags.DEFINE_integer('batch_size', 32, \"Number of images per batch\")\n",
    "tf.app.flags.DEFINE_integer('epochs', 10, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_integer('units', 512, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_float('l1', 0.01, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_float('l2', 0.01, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_string('data_folder', 'aerialsmall', \"Folder with training and validation images\")\n",
    "tf.app.flags.DEFINE_string('save_model_dir', './outputs', \"A folder for saving trained model\")\n",
    "tf.app.flags.DEFINE_string('run_id', '999', 'Identifier of the run')\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    # get hold of the current run\n",
    "    run = Run.get_submitted_run()\n",
    "    print(run)\n",
    "    #train_evaluate(run)\n",
    "  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment\n",
    "\n",
    "**Experiment** is a logical container in an Azure ML Workspace. It hosts run records which can include run metrics and output artifacts from your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'keras-training-on-gpu-cluster'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create remote compute cluster\n",
    "\n",
    "**Creation of the cluster takes approximately 5 minutes.** If the cluster is already in the workspace this code uses it and skips the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating a new compute target...\n",
      "Creating\n",
      "succeeded..........\n",
      "BatchAI wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'allocationState': 'steady', 'allocationStateTransitionTime': '2018-10-10T01:53:46.919000+00:00', 'creationTime': '2018-10-10T01:52:19.223000+00:00', 'currentNodeCount': 1, 'errors': None, 'nodeStateCounts': {'idleNodeCount': 0, 'leavingNodeCount': 0, 'preparingNodeCount': 1, 'runningNodeCount': 0, 'unusableNodeCount': 0}, 'provisioningState': 'succeeded', 'provisioningStateTransitionTime': '2018-10-10T01:52:31.259000+00:00', 'scaleSettings': {'manual': None, 'autoScale': {'maximumNodeCount': 4, 'minimumNodeCount': 1, 'initialNodeCount': 1}}, 'vmPriority': 'dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, BatchAiCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "batchai_cluster_name = ws.name + 'gpucluster'\n",
    "\n",
    "try:\n",
    "    # look for the existing cluster by name\n",
    "    compute_target = ComputeTarget(workspace=ws, name=batchai_cluster_name)\n",
    "    if type(compute_target) is BatchAiCompute:\n",
    "        print('found compute target {}, just use it.'.format(batchai_cluster_name))\n",
    "    else:\n",
    "        print('{} exists but it is not a Batch AI cluster. Please choose a different name.'.format(batchai_cluster_name))\n",
    "except ComputeTargetException:\n",
    "    print('creating a new compute target...')\n",
    "    compute_config = BatchAiCompute.provisioning_configuration(vm_size=\"STANDARD_NC6\", # GPU-based VM\n",
    "                                                                #vm_priority='lowpriority', # optional\n",
    "                                                                autoscale_enabled=True,\n",
    "                                                                cluster_min_nodes=1, \n",
    "                                                                cluster_max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, batchai_cluster_name, compute_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "    # Use the 'status' property to get a detailed status for the current cluster. \n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure datastore\n",
    "The dataset we will use for training has been uploaded to a public Azure blob storage container. We will register this container as the datastore with the workspace. This will create a connection to data for remote compute targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering datastore failed with 400 error code and error message\n",
      "b'{\\n  \"error\": {\\n    \"code\": \"UserError\",\\n    \"message\": \"Another data store with the same name already exists but with different values. Please use patch to update.\",\\n    \"target\": null,\\n    \"details\": [],\\n    \"innerError\": null,\\n    \"debugInfo\": {\\n      \"type\": \"Microsoft.MachineLearning.Common.WebApi.Exceptions.BadRequestException\",\\n      \"message\": \"Another data store with the same name already exists but with different values. Please use patch to update.\",\\n      \"stackTrace\": \"   at Microsoft.MachineLearning.DataStore.EntryPoints.Controllers.DataStoreController.CreateOrUpdate(DataStoreDto dto, Boolean create, Boolean createIfNotExists) in /home/vsts/work/1/s/src/azureml-api/src/DataStore/EntryPoints/Controllers/DataStoreController.cs:line 120\\\\n   at Microsoft.MachineLearning.DataStore.EntryPoints.Controllers.DataStoreController.Create(DataStoreDto dto, Boolean createIfNotExists) in /home/vsts/work/1/s/src/azureml-api/src/DataStore/EntryPoints/Controllers/DataStoreController.cs:line 59\\\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeActionMethodAsync()\\\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeNextActionFilterAsync()\\\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.Rethrow(ActionExecutedContext context)\\\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.Next(State& next, Scope& scope, Object& state, Boolean& isCompleted)\\\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeInnerFilterAsync()\\\\n   at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.InvokeNextExceptionFilterAsync()\",\\n      \"innerException\": null,\\n      \"data\": {},\\n      \"errorResponse\": null\\n    }\\n  },\\n  \"correlation\": {\\n    \"operation\": \"93233bea-44b1447e58fe7500\",\\n    \"request\": \"RvrioOdge4I=\"\\n  }\\n}'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing datastore: aerialsmall\n",
      "aerialsmall AzureBlob azureailabs aerialsmall\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "datastore_name = 'aerialsmall'\n",
    "try:\n",
    "    ds = Datastore.register_azure_blob_container(workspace=ws, datastore_name=datastore_name,\n",
    "                                            container_name='aerialsmall',\n",
    "                                            account_name='azureailabs')\n",
    "    print('Creating new datastore')\n",
    "except:\n",
    "    ds = Datastore(ws, datastore_name)\n",
    "    print('Found existing datastore:', ds.name)\n",
    "   \n",
    "print(ds.name, ds.datastore_type, ds.account_name, ds.container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure data access\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import DataReferenceConfiguration\n",
    "dr = DataReferenceConfiguration(datastore_name=ds.name, \n",
    "                   path_on_datastore=None, \n",
    "                   path_on_compute=datastore_name,\n",
    "                   mode='download', # download files from datastore to compute target\n",
    "                   overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an estimator\n",
    "\n",
    "An estimator object is used to submit the training run.  Create the estimator by running the following code to define:\n",
    "\n",
    "* The name of the estimator object, `est`\n",
    "* The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution. \n",
    "* The compute target.  In this case you will use the Batch AI cluster you created\n",
    "* The training script name, train.py\n",
    "* Parameters required from the training script \n",
    "* Python packages needed for training\n",
    "\n",
    "In this tutorial, the target is the Batch AI cluster. All files in the script folder are uploaded into the cluster nodes for execution. The data_folder is set to use the datastore (`ds.as_download`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_aerialsmall"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.as_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data_folder': ds.as_download(),\n",
    "    '--l1': 0.01,\n",
    "    '--l2': 0.01,\n",
    "    '--units': 512,\n",
    "    '--epochs': 10\n",
    "}\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='train.py',\n",
    "                node_count=1,\n",
    "                process_count_per_node=1,\n",
    "                use_gpu=True,\n",
    "                pip_packages=['h5py','pillow','tensorflow-gpu']\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job to the cluster\n",
    "\n",
    "Run the experiment by submitting the estimator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>keras-training-on-gpu-cluster</td><td>keras-training-on-gpu-cluster_1539138912293</td><td>azureml.scriptrun</td><td>Running</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/952a710c-8d9c-40c1-9fec-f752138cc0b3/resourceGroups/jkaml/providers/Microsoft.MachineLearningServices/workspaces/jkaml/experiments/keras-training-on-gpu-cluster/runs/keras-training-on-gpu-cluster_1539138912293\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: keras-training-on-gpu-cluster,\n",
       "Id: keras-training-on-gpu-cluster_1539138912293,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Running)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = exp.submit(config=est)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the call is asynchronous, it returns a **Preparing** or **Running** state as soon as the job is started.\n",
    "\n",
    "## Monitor a remote run\n",
    "\n",
    "In total, the first run takes **approximately 10 minutes**. But for subsequent runs, as long as the script dependencies don't change, the same image is reused and hence the container start up time is much faster.\n",
    "\n",
    "Here is what's happening while you wait:\n",
    "\n",
    "- **Image creation**: A Docker image is created matching the Python environment specified by the estimator. The image is uploaded to the workspace. This stage happens once for each Python environment since the container is cached for subsequent runs.  During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n",
    "\n",
    "- **Scaling**: If the remote cluster requires more nodes to execute the run than currently available, additional nodes are added automatically. \n",
    "\n",
    "- **Running**: In this stage, the necessary scripts and files are sent to the compute target, then data stores are mounted/copied, then the entry_script is run. While the job is running, stdout and the ./logs directory are streamed to the run history. You can monitor the run's progress using these logs.\n",
    "\n",
    "- **Post-Processing**: The ./outputs directory of the run is copied over to the run history in your workspace so you can access these results.\n",
    "\n",
    "\n",
    "You can check the progress of a running job in multiple ways. This tutorial uses a Jupyter widget as well as a `wait_for_completion` method. \n",
    "\n",
    "### Jupyter widget\n",
    "\n",
    "Watch the progress of the run with a Jupyter widget.  Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes. \n",
    "\n",
    "Note: Currently, there is a problem with RunDetails widget in DSVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cac1560e1d74667800a64db8bd78aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRun()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.train.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get log results upon completion\n",
    "\n",
    "Model training and monitoring happen in the background. Wait until the model has completed training before running more code. Use `wait_for_completion` to show when the model training is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: keras-training-on-gpu-cluster_1539138912293\n",
      "\n",
      "Streaming azureml-logs/60_control_log.txt\n",
      "=========================================\n",
      "\n",
      "Streaming log file azureml-logs/60_control_log.txt\n",
      "Streaming log file azureml-logs/80_driver_log.txt\n",
      "\n",
      "Streaming azureml-logs/80_driver_log.txt\n",
      "========================================\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\n",
      "    8192/94653016 [..............................] - ETA: 6:\n",
      "   40960/94653016 [..............................] - ETA: 4:02\n",
      "  106496/94653016 [..............................] - ETA: 2:\n",
      "  180224/94653016 [..............................] - ETA: 2:\n",
      "  303104/94653016 [..............................] - ETA: 1:\n",
      "  475136/94653016 [..............................] - ETA: 1:\n",
      "  679936/94653016 [..............................] - ETA: 1:\n",
      "  999424/94653016 [..............................] - ETA: 49s\n",
      " 1449984/94653016 [..............................] - ETA: 38\n",
      " 2072576/94653016 [..............................] - ETA: 29\n",
      " 2908160/94653016 [..............................] - ETA: 23\n",
      " 4063232/94653016 [>.............................] - ETA: 17\n",
      " 5578752/94653016 [>.............................] - ETA: 13\n",
      " 7544832/94653016 [=>............................] - ETA: 10\n",
      " 9986048/94653016 [==>...........................] - ETA: 8\n",
      "13164544/94653016 [===>..........................] - ETA: \n",
      "16982016/94653016 [====>.........................] - ETA: \n",
      "20324352/94653016 [=====>........................] - ETA: \n",
      "24223744/94653016 [======>.......................] - ETA: \n",
      "28352512/94653016 [=======>......................] - ETA: \n",
      "32808960/94653016 [=========>....................] - ETA: \n",
      "37363712/94653016 [==========>...................] - ETA: \n",
      "41754624/94653016 [============>.................] - ETA: \n",
      "45981696/94653016 [=============>................] - ETA: \n",
      "48078848/94653016 [==============>...............] - ETA: \n",
      "49913856/94653016 [==============>...............] - ETA: \n",
      "52273152/94653016 [===============>..............] - ETA: \n",
      "56369152/94653016 [================>.............] - ETA: \n",
      "60284928/94653016 [==================>...........] - ETA: \n",
      "62234624/94653016 [==================>...........] - ETA: \n",
      "66101248/94653016 [===================>..........] - ETA: \n",
      "69885952/94653016 [=====================>........] - ETA: \n",
      "73670656/94653016 [======================>.......] - ETA: \n",
      "78413824/94653016 [=======================>......] - ETA: \n",
      "82649088/94653016 [=========================>....] - ETA: \n",
      "87531520/94653016 [==========================>...] - ETA: \n",
      "91561984/94653016 [============================>.] - ETA: \n",
      "94658560/94653016 [==============================] - 2s 0us/step\n",
      "2018-10-10 02:39:38.253431: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2018-10-10 02:39:38.363660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 2fba:00:00.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.09GiB\n",
      "2018-10-10 02:39:38.363702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\n",
      "2018-10-10 02:39:38.636588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2018-10-10 02:39:38.636649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \n",
      "2018-10-10 02:39:38.636661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \n",
      "2018-10-10 02:39:38.636909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10748 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 2fba:00:00.0, compute capability: 3.7)\n",
      "\n",
      " 1/56 [..............................] - ETA: 7:35\n",
      " 2/56 [>.............................] - ETA: 5:00\n",
      " 3/56 [>.............................] - ETA: 4:46\n",
      " 4/56 [=>............................] - ETA: 4:33\n",
      " 5/56 [=>............................] - ETA: 4:25\n",
      " 6/56 [==>...........................] - ETA: 4:25\n",
      " 7/56 [==>...........................] - ETA: 4:28\n",
      " 8/56 [===>..........................] - ETA: 4:37\n",
      " 9/56 [===>..........................] - ETA: 4:36\n",
      "10/56 [====>.........................] - ETA: 4:26\n",
      "11/56 [====>.........................] - ETA: 4:13\n",
      "12/56 [=====>........................] - ETA: 4:04\n",
      "13/56 [=====>........................] - ETA: 3:57\n",
      "14/56 [======>.......................] - ETA: 3:50\n",
      "15/56 [=======>......................] - ETA: 3:42\n",
      "16/56 [=======>......................] - ETA: 3:36\n",
      "17/56 [========>.....................] - ETA: 3:33\n",
      "18/56 [========>.....................] - ETA: 3:26\n",
      "19/56 [=========>....................] - ETA: 3:20\n",
      "20/56 [=========>....................] - ETA: 3:14\n",
      "21/56 [==========>...................] - ETA: 3:07\n",
      "22/56 [==========>...................] - ETA: 3:01\n",
      "23/56 [===========>..................] - ETA: 2:56\n",
      "24/56 [===========>..................] - ETA: 2:53\n",
      "25/56 [============>.................] - ETA: 2:47\n",
      "26/56 [============>.................] - ETA: 2:45\n",
      "27/56 [=============>................] - ETA: 2:43\n",
      "28/56 [==============>...............] - ETA: 2:41\n",
      "29/56 [==============>...............] - ETA: 2:35\n",
      "30/56 [===============>..............] - ETA: 2:31\n",
      "31/56 [===============>..............] - ETA: 2:27\n",
      "32/56 [================>.............] - ETA: 2:22\n",
      "33/56 [================>.............] - ETA: 2:17\n",
      "34/56 [=================>............] - ETA: 2:11\n",
      "35/56 [=================>............] - ETA: 2:07\n",
      "36/56 [==================>...........] - ETA: 2:02\n",
      "37/56 [==================>...........] - ETA: 1:55\n",
      "38/56 [===================>..........] - ETA: 1:50\n",
      "39/56 [===================>..........] - ETA: 1:45\n",
      "40/56 [====================>.........] - ETA: 1:39\n",
      "41/56 [====================>.........] - ETA: 1:34\n",
      "42/56 [=====================>........] - ETA: 1:28\n",
      "43/56 [======================>.......] - ETA: 1:22\n",
      "44/56 [======================>.......] - ETA: 1:15\n",
      "45/56 [=======================>......] - ETA: 1:09\n",
      "46/56 [=======================>......] - ETA: 1:03\n",
      "47/56 [========================>.....] - ETA: 56s \n",
      "48/56 [========================>.....] - ETA: 50s\n",
      "49/56 [=========================>....] - ETA: 43s\n",
      "50/56 [=========================>....] - ETA: 37s\n",
      "51/56 [==========================>...] - ETA: 31s\n",
      "52/56 [==========================>...] - ETA: 25s\n",
      "53/56 [===========================>..] - ETA: 18s\n",
      "54/56 [===========================>..] - ETA: 12s\n",
      "55/56 [============================>.] - ETA: 6s \n",
      "56/56 [==============================] - 352s 6s/step\n",
      "\n",
      " 1/10 [==>...........................] - ETA: 1:48\n",
      " 2/10 [=====>........................] - ETA: 1:12\n",
      " 3/10 [========>.....................] - ETA: 55s \n",
      " 4/10 [===========>..................] - ETA: 42s\n",
      " 5/10 [==============>...............] - ETA: 32s\n",
      " 6/10 [=================>............] - ETA: 25s\n",
      " 7/10 [====================>.........] - ETA: 19s\n",
      " 8/10 [=======================>......] - ETA: 12s\n",
      " 9/10 [==========================>...] - ETA: 6s \n",
      "10/10 [==============================] - 63s 6s/step\n",
      "Starting training\n",
      "Train on 3535 samples, validate on 618 samples\n",
      "Epoch 1/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 36s - loss: 4.2509 - acc: 0.12\n",
      " 896/3535 [======>.......................] - ETA: 2s - loss: 2.3470 - acc: 0.7801\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 2.1034 - acc: 0.824\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 1.9497 - acc: 0.846\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 1.8185 - acc: 0.864\n",
      "3535/3535 [==============================] - 1s 286us/step - loss: 1.7995 - acc: 0.8665 - val_loss: 1.2760 - val_acc: 0.9159\n",
      "Epoch 2/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 1.3597 - acc: 0.921\n",
      " 768/3535 [=====>........................] - ETA: 0s - loss: 1.2022 - acc: 0.936\n",
      "1600/3535 [============>.................] - ETA: 0s - loss: 1.1408 - acc: 0.934\n",
      "2432/3535 [===================>..........] - ETA: 0s - loss: 1.0819 - acc: 0.933\n",
      "3264/3535 [==========================>...] - ETA: 0s - loss: 1.0384 - acc: 0.932\n",
      "3535/3535 [==============================] - 0s 70us/step - loss: 1.0259 - acc: 0.9310 - val_loss: 0.8092 - val_acc: 0.9337\n",
      "Epoch 3/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.8177 - acc: 0.906\n",
      " 896/3535 [======>.......................] - ETA: 0s - loss: 0.7456 - acc: 0.936\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 0.7029 - acc: 0.941\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.6838 - acc: 0.937\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 0.6590 - acc: 0.935\n",
      "3535/3535 [==============================] - 0s 68us/step - loss: 0.6515 - acc: 0.9366 - val_loss: 0.5435 - val_acc: 0.9320\n",
      "Epoch 4/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.4679 - acc: 0.984\n",
      " 896/3535 [======>.......................] - ETA: 0s - loss: 0.5099 - acc: 0.952\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 0.4855 - acc: 0.950\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.4803 - acc: 0.941\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 0.4731 - acc: 0.940\n",
      "3535/3535 [==============================] - 0s 67us/step - loss: 0.4716 - acc: 0.9406 - val_loss: 0.4585 - val_acc: 0.9272\n",
      "Epoch 5/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.5846 - acc: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 896/3535 [======>.......................] - ETA: 0s - loss: 0.4222 - acc: 0.9386\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 0.4145 - acc: 0.9427\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.4048 - acc: 0.9449\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 0.4042 - acc: 0.9446\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3535/3535 [==============================] - 0s 67us/step - loss: 0.4007 - acc: 0.9457 - val_loss: 0.3792 - val_acc: 0.9320\n",
      "Epoch 6/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.3609 - acc: 0.9531\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      " 896/3535 [======>.......................] - ETA: 0s - loss: 0.3374 - acc: 0.9554\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 0.3538 - acc: 0.9439\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.3476 - acc: 0.9473\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 0.3604 - acc: 0.9428\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3535/3535 [==============================] - 0s 68us/step - loss: 0.3599 - acc: 0.9426 - val_loss: 0.3556 - val_acc: 0.9353\n",
      "Epoch 7/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.2888 - acc: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      " 896/3535 [======>.......................] - ETA: 0s - loss: 0.3382 - acc: 0.9420\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 0.3381 - acc: 0.9444\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.3364 - acc: 0.9449\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 0.3322 - acc: 0.9455\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3535/3535 [==============================] - 0s 67us/step - loss: 0.3304 - acc: 0.9465 - val_loss: 0.3437 - val_acc: 0.9256\n",
      "Epoch 8/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.3310 - acc: 0.9375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      " 896/3535 [======>.......................] - ETA: 0s - loss: 0.3182 - acc: 0.9542\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 0.3096 - acc: 0.9531\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.3139 - acc: 0.9492\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 0.3095 - acc: 0.9496\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3535/3535 [==============================] - 0s 67us/step - loss: 0.3118 - acc: 0.9491 - val_loss: 0.3573 - val_acc: 0.9223\n",
      "Epoch 9/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.3940 - acc: 0.9062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      " 896/3535 [======>.......................] - ETA: 0s - loss: 0.3213 - acc: 0.9353\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 0.3045 - acc: 0.9462\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.2982 - acc: 0.9508\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 0.3012 - acc: 0.9487\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3535/3535 [==============================] - 0s 68us/step - loss: 0.2989 - acc: 0.9499 - val_loss: 0.3363 - val_acc: 0.9256\n",
      "Epoch 10/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.4056 - acc: 0.8594\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      " 896/3535 [======>.......................] - ETA: 0s - loss: 0.2890 - acc: 0.9475\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 0.2984 - acc: 0.9456\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.2876 - acc: 0.9512\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 0.2821 - acc: 0.9522\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3535/3535 [==============================] - 0s 67us/step - loss: 0.2857 - acc: 0.9508 - val_loss: 0.3495 - val_acc: 0.9288\n",
      "Epoch 11/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.2498 - acc: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      " 896/3535 [======>.......................] - ETA: 0s - loss: 0.2704 - acc: 0.9576\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 0.2774 - acc: 0.9508\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.2762 - acc: 0.9512\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 0.2711 - acc: 0.9540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3535/3535 [==============================] - 0s 67us/step - loss: 0.2727 - acc: 0.9539 - val_loss: 0.3372 - val_acc: 0.9304\n",
      "Epoch 12/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.1910 - acc: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      " 832/3535 [======>.......................] - ETA: 0s - loss: 0.2736 - acc: 0.9555\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1664/3535 [=============>................] - ETA: 0s - loss: 0.2831 - acc: 0.9507\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "2496/3535 [====================>.........] - ETA: 0s - loss: 0.2703 - acc: 0.9547\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3328/3535 [===========================>..] - ETA: 0s - loss: 0.2725 - acc: 0.9522\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3535/3535 [==============================] - 0s 68us/step - loss: 0.2724 - acc: 0.9516 - val_loss: 0.3190 - val_acc: 0.9385\n",
      "Epoch 13/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.3708 - acc: 0.9219\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      " 896/3535 [======>.......................] - ETA: 0s - loss: 0.2445 - acc: 0.9565\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 0.2447 - acc: 0.9566\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.2569 - acc: 0.9535\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 0.2560 - acc: 0.9531\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "3535/3535 [==============================] - 0s 67us/step - loss: 0.2585 - acc: 0.9528 - val_loss: 0.3215 - val_acc: 0.9320\n",
      "Epoch 14/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.2365 - acc: 0.9531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 960/3535 [=======>......................] - ETA: 0s - loss: 0.2477 - acc: 0.959\n",
      "1792/3535 [==============>...............] - ETA: 0s - loss: 0.2512 - acc: 0.955\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.2502 - acc: 0.958\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 0.2505 - acc: 0.958\n",
      "3535/3535 [==============================] - 0s 71us/step - loss: 0.2502 - acc: 0.9584 - val_loss: 0.2833 - val_acc: 0.9401\n",
      "Epoch 15/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.1794 - acc: 0.984\n",
      " 896/3535 [======>.......................] - ETA: 0s - loss: 0.2365 - acc: 0.959\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 0.2422 - acc: 0.957\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.2419 - acc: 0.958\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 0.2484 - acc: 0.957\n",
      "3535/3535 [==============================] - 0s 67us/step - loss: 0.2480 - acc: 0.9564 - val_loss: 0.2931 - val_acc: 0.9369\n",
      "Epoch 16/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.1920 - acc: 0.984\n",
      " 896/3535 [======>.......................] - ETA: 0s - loss: 0.2293 - acc: 0.964\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 0.2437 - acc: 0.958\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.2346 - acc: 0.963\n",
      "3328/3535 [===========================>..] - ETA: 0s - loss: 0.2391 - acc: 0.960\n",
      "3535/3535 [==============================] - 0s 67us/step - loss: 0.2419 - acc: 0.9590 - val_loss: 0.3298 - val_acc: 0.9288\n",
      "Epoch 17/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.3377 - acc: 0.921\n",
      " 896/3535 [======>.......................] - ETA: 0s - loss: 0.2570 - acc: 0.953\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 0.2415 - acc: 0.961\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.2353 - acc: 0.961\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 0.2392 - acc: 0.960\n",
      "3535/3535 [==============================] - 0s 67us/step - loss: 0.2384 - acc: 0.9607 - val_loss: 0.3392 - val_acc: 0.9288\n",
      "Epoch 18/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.2600 - acc: 0.921\n",
      " 896/3535 [======>.......................] - ETA: 0s - loss: 0.2441 - acc: 0.952\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 0.2420 - acc: 0.954\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.2316 - acc: 0.959\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 0.2321 - acc: 0.960\n",
      "3535/3535 [==============================] - 0s 67us/step - loss: 0.2317 - acc: 0.9601 - val_loss: 0.2980 - val_acc: 0.9369\n",
      "Epoch 19/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.1744 - acc: 0.968\n",
      " 896/3535 [======>.......................] - ETA: 0s - loss: 0.2289 - acc: 0.956\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 0.2231 - acc: 0.961\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.2266 - acc: 0.960\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 0.2311 - acc: 0.959\n",
      "3535/3535 [==============================] - 0s 67us/step - loss: 0.2315 - acc: 0.9590 - val_loss: 0.2749 - val_acc: 0.9417\n",
      "Epoch 20/20\n",
      "\n",
      "  64/3535 [..............................] - ETA: 0s - loss: 0.2660 - acc: 0.937\n",
      " 896/3535 [======>.......................] - ETA: 0s - loss: 0.2276 - acc: 0.966\n",
      "1728/3535 [=============>................] - ETA: 0s - loss: 0.2126 - acc: 0.970\n",
      "2560/3535 [====================>.........] - ETA: 0s - loss: 0.2224 - acc: 0.966\n",
      "3392/3535 [===========================>..] - ETA: 0s - loss: 0.2223 - acc: 0.964\n",
      "3535/3535 [==============================] - 0s 67us/step - loss: 0.2241 - acc: 0.9641 - val_loss: 0.2897 - val_acc: 0.9434\n",
      "Training completed.\n",
      "\n",
      "\n",
      "The experiment failed. Finalizing run...\n",
      "Logging experiment finalizing status in history service\n",
      "Traceback (most recent call last):\n",
      "  File \"azureml-setup/context_manager_injector.py\", line 70, in execute_with_context\n",
      "    runpy.run_path(sys.argv[0], globals(), run_name=\"__main__\")\n",
      "  File \"/azureml-envs/azureml_93722c4a961d4d2680ff2fb306b0658c/lib/python3.6/runpy.py\", line 263, in run_path\n",
      "    pkg_name=pkg_name, script_name=fname)\n",
      "  File \"/azureml-envs/azureml_93722c4a961d4d2680ff2fb306b0658c/lib/python3.6/runpy.py\", line 96, in _run_module_code\n",
      "    mod_name, mod_spec, pkg_name, script_name)\n",
      "  File \"/azureml-envs/azureml_93722c4a961d4d2680ff2fb306b0658c/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"train.py\", line 151, in <module>\n",
      "    tf.app.run()\n",
      "  File \"/azureml-envs/azureml_93722c4a961d4d2680ff2fb306b0658c/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
      "    _sys.exit(main(argv))\n",
      "  File \"train.py\", line 147, in main\n",
      "    train_evaluate(run)\n",
      "  File \"train.py\", line 125, in train_evaluate\n",
      "    model_file = os.path.join('outputs', run.run_id + '.hd5')\n",
      "AttributeError: '_SubmittedRun' object has no attribute 'run_id'\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"azureml-setup/context_manager_injector.py\", line 126, in <module>\n",
      "    execute_with_context(cm_objects, options.invocation)\n",
      "  File \"azureml-setup/context_manager_injector.py\", line 70, in execute_with_context\n",
      "    runpy.run_path(sys.argv[0], globals(), run_name=\"__main__\")\n",
      "  File \"/azureml-envs/azureml_93722c4a961d4d2680ff2fb306b0658c/lib/python3.6/runpy.py\", line 263, in run_path\n",
      "    pkg_name=pkg_name, script_name=fname)\n",
      "  File \"/azureml-envs/azureml_93722c4a961d4d2680ff2fb306b0658c/lib/python3.6/runpy.py\", line 96, in _run_module_code\n",
      "    mod_name, mod_spec, pkg_name, script_name)\n",
      "  File \"/azureml-envs/azureml_93722c4a961d4d2680ff2fb306b0658c/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"train.py\", line 151, in <module>\n",
      "    tf.app.run()\n",
      "  File \"/azureml-envs/azureml_93722c4a961d4d2680ff2fb306b0658c/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
      "    _sys.exit(main(argv))\n",
      "  File \"train.py\", line 147, in main\n",
      "    train_evaluate(run)\n",
      "  File \"train.py\", line 125, in train_evaluate\n",
      "    model_file = os.path.join('outputs', run.run_id + '.hd5')\n",
      "AttributeError: '_SubmittedRun' object has no attribute 'run_id'\n"
     ]
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=True) # specify True for a verbose log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display run results\n",
    "\n",
    "The training has completed. You can see the logs generated during the run by executing `Run.get_file_names()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azureml-logs/20_image_build_log.txt', 'azureml-logs/60_control_log.txt', 'azureml-logs/80_driver_log.txt', 'outputs/999.hd5', 'driver_log', 'azureml-logs/azureml.log']\n"
     ]
    }
   ],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file('azureml-logs/20_image_build_log.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
